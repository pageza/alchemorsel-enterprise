# Docker Compose Override for Ollama Development
# This file provides development-specific configurations for Ollama service
# Use with: docker-compose -f docker-compose.services.yml -f deployments/ollama/docker-compose.override.yml

version: '3.8'

services:
  ollama:
    # Development-specific environment variables
    environment:
      # Enable debug mode
      OLLAMA_DEBUG: "true"
      
      # Development model configuration
      OLLAMA_PRELOAD_MODELS: "llama3.2:3b,llama3.2:1b"
      
      # Faster model loading for development
      OLLAMA_MODEL_TIMEOUT: 300
      OLLAMA_MAX_RETRIES: 2
      
      # Development resource limits
      OLLAMA_NUM_PARALLEL: 1
      OLLAMA_MAX_LOADED_MODELS: 1
      
      # Extended health check timeout for slow development machines
      OLLAMA_HEALTH_TIMEOUT: 60
    
    # Development resource constraints (lighter for local development)
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    
    # Development volumes for easier debugging
    volumes:
      # Persistent model storage
      - ollama_models:/root/.ollama
      # Configuration and scripts (read-only)
      - ./deployments/ollama/scripts:/scripts:ro
      - ./deployments/ollama/health:/health:ro
      # Development logs volume for easier access
      - ./logs/ollama:/var/log/ollama
    
    # Development health check with more lenient timing
    healthcheck:
      test: [
        "CMD", 
        "/health/healthcheck.sh"
      ]
      interval: 45s
      timeout: 20s
      retries: 5
      start_period: 300s  # Extended for slow development machines
    
    # Development labels
    labels:
      - "com.docker.compose.service=ollama"
      - "monitoring.enable=true"
      - "ai.model=llama3.2:3b"
      - "ai.provider=ollama"
      - "environment=development"
      - "auto-restart=true"

  # Development-specific API service overrides for Ollama integration
  api:
    environment:
      # Enhanced AI debugging
      ALCHEMORSEL_AI_PROVIDER: ollama
      ALCHEMORSEL_OLLAMA_HOST: http://ollama:11434
      ALCHEMORSEL_OLLAMA_MODEL: llama3.2:3b
      ALCHEMORSEL_OLLAMA_TIMEOUT: 45s
      
      # Development AI configuration
      ALCHEMORSEL_AI_ENABLE_CACHE: "true"
      ALCHEMORSEL_AI_CACHE_TTL: "30m"
      ALCHEMORSEL_AI_MAX_TOKENS: 1500
      ALCHEMORSEL_AI_TEMPERATURE: 0.7
      
      # Debug logging for AI operations
      ALCHEMORSEL_APP_DEBUG: "true"
      ALCHEMORSEL_APP_LOG_LEVEL: "debug"
    
    # API depends on Ollama in development
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy

volumes:
  # Development-specific volume configurations
  ollama_models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/ollama
    labels:
      - "service=ollama"
      - "type=models"
      - "environment=development"