# AlertManager Configuration for Alchemorsel v3
# Defines routing, grouping, and notification policies for alerts

global:
  # Default SMTP configuration
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@alchemorsel.com'
  smtp_auth_username: 'alerts@alchemorsel.com'
  smtp_auth_password_file: '/etc/alertmanager/secrets/smtp_password'
  smtp_require_tls: true
  
  # Default notification template
  smtp_subject: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] Alchemorsel Alert'
  
  # Slack webhook URL for general notifications
  slack_api_url_file: '/etc/alertmanager/secrets/slack_webhook'
  
  # PagerDuty integration key
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

# Template files for custom notification formatting
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree for alert routing and grouping
route:
  # Root route - default configuration
  group_by: ['cluster', 'service', 'alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'platform-team'
  
  routes:
    # === Critical Alerts - Immediate Response ===
    - match:
        severity: critical
      group_by: ['cluster', 'service']
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 5m
      receiver: 'critical-alerts'
      routes:
        # Service down alerts - highest priority
        - match_re:
            alertname: '.*ServiceDown|.*Down'
          receiver: 'service-down'
          continue: true
        
        # SLO breach alerts
        - match_re:
            alertname: '.*SLOBreach|.*SLA.*'
          receiver: 'slo-breach'
          continue: true
        
        # Security alerts
        - match:
            team: security
          receiver: 'security-team'
          continue: true
        
        # Database critical issues
        - match:
            service: postgresql
        receiver: 'database-team'
        continue: true
    
    # === Warning Alerts - Standard Response ===
    - match:
        severity: warning
      group_by: ['cluster', 'service', 'alertname']
      group_wait: 5m
      group_interval: 10m
      repeat_interval: 2h
      receiver: 'warning-alerts'
      routes:
        # Performance warnings
        - match_re:
            alertname: '.*Latency.*|.*Performance.*'
          receiver: 'performance-team'
        
        # Capacity warnings
        - match_re:
            alertname: '.*Usage.*|.*Capacity.*'
          receiver: 'infrastructure-team'
        
        # Business metric warnings
        - match:
            team: product
          receiver: 'product-team'
    
    # === Team-specific Routes ===
    
    # AI/ML team alerts
    - match:
        team: ai
      receiver: 'ai-team'
      group_by: ['service', 'alertname']
      repeat_interval: 1h
    
    # Platform team alerts (default catch-all)
    - match:
        team: platform
      receiver: 'platform-team'
    
    # === Development Environment ===
    - match:
        environment: development
      receiver: 'dev-alerts'
      group_wait: 5m
      repeat_interval: 4h
    
    # === Testing/Staging Environment ===
    - match:
        environment: staging
      receiver: 'staging-alerts'
      group_wait: 2m
      repeat_interval: 1h

# Silence configuration
inhibit_rules:
  # Silence warning alerts when critical alerts are firing for the same service
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['service', 'instance']
  
  # Silence individual service alerts when entire cluster is down
  - source_match:
      alertname: 'ClusterDown'
    target_match_re:
      alertname: '.*ServiceDown|.*Unavailable'
    equal: ['cluster']
  
  # Silence downstream alerts when database is down
  - source_match:
      alertname: 'DatabaseDown'
    target_match_re:
      alertname: '.*ConnectionFailed|.*QueryTimeout'
    equal: ['cluster']

# Receiver configurations
receivers:
  # === Critical Alert Receivers ===
  
  - name: 'critical-alerts'
    pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/secrets/pagerduty_routing_key'
        description: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        details:
          summary: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
          description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
          severity: '{{ .CommonLabels.severity }}'
          service: '{{ .CommonLabels.service }}'
          runbook: '{{ range .Alerts }}{{ .Annotations.runbook }}{{ end }}'
        severity: 'critical'
    
    slack_configs:
      - api_url_file: '/etc/alertmanager/secrets/slack_webhook_critical'
        channel: '#alerts-critical'
        title: 'üö® CRITICAL ALERT'
        text: |
          {{ range .Alerts }}
          *Service:* {{ .Labels.service }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Impact:* {{ .Annotations.impact }}
          *Action Required:* {{ .Annotations.action }}
          *Runbook:* {{ .Annotations.runbook }}
          {{ end }}
        send_resolved: true
        actions:
          - type: button
            text: 'Acknowledge'
            url: 'https://alertmanager.alchemorsel.com/#/silences/new'
          - type: button
            text: 'Runbook'
            url: '{{ range .Alerts }}{{ .Annotations.runbook }}{{ end }}'
    
    email_configs:
      - to: 'oncall@alchemorsel.com,platform-leads@alchemorsel.com'
        subject: 'üö® CRITICAL: {{ .GroupLabels.alertname }} - {{ .GroupLabels.service }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Service: {{ .Labels.service }}
          Severity: {{ .Labels.severity }}
          
          Description: {{ .Annotations.description }}
          Impact: {{ .Annotations.impact }}
          Action Required: {{ .Annotations.action }}
          
          Runbook: {{ .Annotations.runbook }}
          
          Started: {{ .StartsAt }}
          {{ if .EndsAt }}Ended: {{ .EndsAt }}{{ end }}
          {{ end }}

  - name: 'service-down'
    pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/secrets/pagerduty_routing_key'
        severity: 'critical'
        description: 'Service Down: {{ .CommonLabels.service }}'
    
    # Immediate phone call for service outages
    webhook_configs:
      - url: 'https://api.twilio.com/2010-04-01/Accounts/YOUR_ACCOUNT_SID/Calls.json'
        http_config:
          basic_auth:
            username: 'YOUR_ACCOUNT_SID'
            password_file: '/etc/alertmanager/secrets/twilio_token'
        send_resolved: false

  - name: 'slo-breach'
    slack_configs:
      - api_url_file: '/etc/alertmanager/secrets/slack_webhook_slo'
        channel: '#sla-violations'
        title: 'üìä SLO BREACH DETECTED'
        text: |
          üö® *SLA/SLO Violation*
          {{ range .Alerts }}
          *Service:* {{ .Labels.service }}
          *SLO Type:* {{ .Labels.slo_type }}
          *Current Value:* {{ .Annotations.description }}
          *Financial Impact:* Potential customer refunds and reputation damage
          *Required Action:* {{ .Annotations.action }}
          {{ end }}

  - name: 'security-team'
    pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/secrets/pagerduty_security_key'
        severity: 'critical'
    
    email_configs:
      - to: 'security-team@alchemorsel.com,ciso@alchemorsel.com'
        subject: 'üîí SECURITY ALERT: {{ .GroupLabels.alertname }}'
    
    slack_configs:
      - api_url_file: '/etc/alertmanager/secrets/slack_webhook_security'
        channel: '#security-alerts'
        title: 'üîí SECURITY ALERT'

  # === Warning Alert Receivers ===
  
  - name: 'warning-alerts'
    slack_configs:
      - api_url_file: '/etc/alertmanager/secrets/slack_webhook'
        channel: '#alerts'
        title: '‚ö†Ô∏è Warning Alert'
        text: |
          {{ range .Alerts }}
          *Service:* {{ .Labels.service }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          {{ end }}
        send_resolved: true

  - name: 'performance-team'
    email_configs:
      - to: 'performance-team@alchemorsel.com'
        subject: '‚ö†Ô∏è Performance Issue: {{ .GroupLabels.alertname }}'
    
    slack_configs:
      - channel: '#performance'
        title: 'üìà Performance Alert'

  - name: 'infrastructure-team'
    email_configs:
      - to: 'infrastructure@alchemorsel.com'
        subject: '‚ö†Ô∏è Infrastructure Alert: {{ .GroupLabels.alertname }}'
    
    slack_configs:
      - channel: '#infrastructure'
        title: 'üèóÔ∏è Infrastructure Alert'

  - name: 'database-team'
    pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/secrets/pagerduty_db_key'
        severity: 'critical'
    
    email_configs:
      - to: 'dba-team@alchemorsel.com'
        subject: 'üóÑÔ∏è Database Critical: {{ .GroupLabels.alertname }}'

  # === Team-specific Receivers ===
  
  - name: 'ai-team'
    email_configs:
      - to: 'ai-team@alchemorsel.com'
        subject: 'ü§ñ AI Service Alert: {{ .GroupLabels.alertname }}'
    
    slack_configs:
      - channel: '#ai-alerts'
        title: 'ü§ñ AI Service Alert'

  - name: 'product-team'
    email_configs:
      - to: 'product-team@alchemorsel.com'
        subject: 'üìä Business Metrics Alert: {{ .GroupLabels.alertname }}'
    
    slack_configs:
      - channel: '#product-alerts'
        title: 'üìä Business Metrics Alert'

  - name: 'platform-team'
    email_configs:
      - to: 'platform-team@alchemorsel.com'
        subject: 'üõ†Ô∏è Platform Alert: {{ .GroupLabels.alertname }}'
    
    slack_configs:
      - channel: '#platform'
        title: 'üõ†Ô∏è Platform Alert'

  # === Environment-specific Receivers ===
  
  - name: 'dev-alerts'
    slack_configs:
      - channel: '#dev-alerts'
        title: 'üîß Development Alert'
        send_resolved: false

  - name: 'staging-alerts'
    slack_configs:
      - channel: '#staging-alerts'
        title: 'üé≠ Staging Alert'

# Time-based routing (future enhancement)
time_intervals:
  - name: 'business-hours'
    time_intervals:
      - times:
          - start_time: '09:00'
            end_time: '18:00'
        weekdays: ['monday:friday']
        location: 'America/New_York'
  
  - name: 'off-hours'
    time_intervals:
      - times:
          - start_time: '18:00'
            end_time: '09:00'
        weekdays: ['monday:friday']
        location: 'America/New_York'
      - weekdays: ['saturday', 'sunday']