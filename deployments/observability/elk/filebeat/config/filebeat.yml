# Filebeat Configuration for Alchemorsel v3
# Collects logs from various sources and sends them to Logstash

filebeat.config:
  modules:
    path: ${path.config}/modules.d/*.yml
    reload.enabled: true
    reload.period: 10s

filebeat.autodiscover:
  providers:
    - type: docker
      hints.enabled: true
      hints.default_config:
        type: container
        paths:
          - /var/lib/docker/containers/${data.docker.container.id}/*.log

# Input configurations
filebeat.inputs:
  # Application logs from containers
  - type: container
    enabled: true
    paths:
      - '/var/lib/docker/containers/*/*.log'
    
    # Container-specific parsing
    processors:
      - add_docker_metadata:
          host: "unix:///var/run/docker.sock"
      
      - script:
          lang: javascript
          id: container_parser
          source: >
            function process(event) {
              var containerName = event.Get("container.name");
              
              // Add service identification based on container name
              if (containerName) {
                if (containerName.includes("alchemorsel-api")) {
                  event.Put("service.name", "alchemorsel-api");
                  event.Put("service.type", "application");
                } else if (containerName.includes("postgres")) {
                  event.Put("service.name", "postgresql");
                  event.Put("service.type", "database");
                } else if (containerName.includes("redis")) {
                  event.Put("service.name", "redis");
                  event.Put("service.type", "cache");
                } else if (containerName.includes("ollama")) {
                  event.Put("service.name", "ollama");
                  event.Put("service.type", "ai");
                } else if (containerName.includes("nginx")) {
                  event.Put("service.name", "nginx");
                  event.Put("service.type", "proxy");
                }
              }
              
              return event;
            }
    
    # Include/exclude patterns
    include_lines: ['^{.*}$', '^[0-9]{4}-[0-9]{2}-[0-9]{2}']
    exclude_lines: ['^DEBUG', '^\\s*$']
    exclude_files: ['\.gz$']
    
    # Multiline pattern for stack traces
    multiline.pattern: '^[[:space:]]+(at|\.{3})[[:space:]]+\b|^Caused by:'
    multiline.negate: false
    multiline.match: after
    multiline.max_lines: 500
    multiline.timeout: 5s

  # System logs
  - type: log
    enabled: true
    paths:
      - /var/log/syslog
      - /var/log/auth.log
      - /var/log/kern.log
    fields:
      service.type: system
      log.source: system
    fields_under_root: true

  # Application specific log files
  - type: log
    enabled: true
    paths:
      - /app/logs/*.log
      - /app/logs/*/*.log
    fields:
      service.name: alchemorsel-api
      service.type: application
      log.source: file
    fields_under_root: true
    
    # JSON decoding for structured logs
    json.keys_under_root: true
    json.add_error_key: true
    json.message_key: message

  # Nginx access logs
  - type: log
    enabled: true
    paths:
      - /var/log/nginx/access.log
      - /var/log/nginx/error.log
    fields:
      service.name: nginx
      service.type: proxy
      log.source: nginx
    fields_under_root: true

# Processors for data enrichment
processors:
  # Add hostname
  - add_host_metadata:
      when.not.contains.tags: forwarded

  # Add Docker metadata
  - add_docker_metadata: ~

  # Add Kubernetes metadata (if running on K8s)
  - add_kubernetes_metadata:
      host: ${NODE_NAME}
      matchers:
        - logs_path:
            logs_path: "/var/log/containers/"

  # Add environment information
  - add_fields:
      target: environment
      fields:
        name: production
        cluster: alchemorsel-production
        region: us-east-1

  # Drop unnecessary fields to reduce data volume
  - drop_fields:
      fields: ["agent", "ecs", "input", "orchestrator"]

  # Add timestamp parsing
  - timestamp:
      field: "@timestamp"
      layouts:
        - '2006-01-02T15:04:05.000Z'
        - '2006-01-02T15:04:05Z'
      test:
        - '2024-01-15T14:23:45.123Z'

# Output configuration
output:
  # Primary output to Logstash
  logstash:
    hosts: ["logstash:5044"]
    worker: 2
    compression_level: 3
    bulk_max_size: 2048
    timeout: 30s
    
    # Load balancing
    loadbalance: true
    
    # Retry configuration
    backoff.init: 1s
    backoff.max: 60s
    max_retries: 3

  # Fallback output to Elasticsearch (if Logstash is down)
  # elasticsearch:
  #   hosts: ["elasticsearch:9200"]
  #   index: "filebeat-fallback-%{+yyyy.MM.dd}"
  #   when.not.has_fields: ["logstash_processed"]

# Logging configuration
logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat.log
  keepfiles: 7
  permissions: 0644

# Monitoring
monitoring:
  enabled: true
  elasticsearch:
    hosts: ["elasticsearch:9200"]
    index: ".monitoring-beats"

# Security (for production)
ssl.verification_mode: none

# Performance tuning
queue:
  mem:
    events: 4096
    flush.min_events: 512
    flush.timeout: 5s

# Modules configuration
filebeat.modules:
  - module: system
    syslog:
      enabled: true
      var.paths: ["/var/log/syslog*"]
    auth:
      enabled: true
      var.paths: ["/var/log/auth.log*"]

  - module: nginx
    access:
      enabled: true
      var.paths: ["/var/log/nginx/access.log*"]
    error:
      enabled: true
      var.paths: ["/var/log/nginx/error.log*"]

  - module: postgresql
    log:
      enabled: true
      var.paths: ["/var/log/postgresql/*.log"]

# HTTP endpoint for health checks
http:
  enabled: true
  host: 0.0.0.0
  port: 5066

# Path configuration
path:
  home: /usr/share/filebeat
  config: ${path.home}
  data: ${path.home}/data
  logs: ${path.home}/logs