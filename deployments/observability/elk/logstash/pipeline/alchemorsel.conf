# Alchemorsel Logstash Pipeline Configuration
# Processes and transforms application logs

input {
  # Beats input for Filebeat
  beats {
    port => 5044
    type => "beats"
  }
  
  # Direct TCP input for applications
  tcp {
    port => 5000
    codec => json_lines
    type => "tcp"
  }
  
  # HTTP input for webhook logs
  http {
    port => 8080
    codec => json
    type => "webhook"
  }
}

filter {
  # Add timestamp if not present
  if ![timestamp] {
    mutate {
      add_field => { "timestamp" => "%{@timestamp}" }
    }
  }
  
  # Parse container logs from Docker
  if [log][file][path] =~ /\/var\/lib\/docker\/containers\// {
    # Extract container ID from path
    grok {
      match => { "[log][file][path]" => "/var/lib/docker/containers/%{DATA:container_id}/%{GREEDYDATA}" }
    }
    
    # Parse Docker JSON log format
    json {
      source => "message"
      target => "docker"
    }
    
    # Extract application logs from Docker stdout/stderr
    if [docker][log] {
      mutate {
        replace => { "message" => "%{[docker][log]}" }
      }
    }
  }
  
  # Process Alchemorsel API logs
  if [container][name] =~ /alchemorsel-api/ or [service] == "alchemorsel-api" {
    # Parse structured JSON logs
    json {
      source => "message"
      target => "app"
    }
    
    # Extract HTTP request logs
    if [app][http] {
      mutate {
        add_field => {
          "http_method" => "%{[app][http][method]}"
          "http_path" => "%{[app][http][path]}"
          "http_status" => "%{[app][http][status]}"
          "response_time_ms" => "%{[app][http][duration_ms]}"
          "user_agent" => "%{[app][http][user_agent]}"
          "remote_ip" => "%{[app][http][remote_ip]}"
        }
      }
      
      # Convert response time to number
      mutate {
        convert => { "response_time_ms" => "float" }
        convert => { "http_status" => "integer" }
      }
    }
    
    # Extract error information
    if [app][error] {
      mutate {
        add_field => {
          "error_message" => "%{[app][error][message]}"
          "error_type" => "%{[app][error][type]}"
          "error_stack" => "%{[app][error][stack]}"
        }
        add_tag => ["error"]
      }
    }
    
    # Extract trace information
    if [app][trace] {
      mutate {
        add_field => {
          "trace_id" => "%{[app][trace][trace_id]}"
          "span_id" => "%{[app][trace][span_id]}"
          "parent_span_id" => "%{[app][trace][parent_span_id]}"
        }
      }
    }
    
    # Extract user information
    if [app][user] {
      mutate {
        add_field => {
          "user_id" => "%{[app][user][id]}"
          "user_email" => "%{[app][user][email]}"
        }
      }
    }
    
    # Extract business metrics
    if [app][business] {
      mutate {
        add_field => {
          "business_event" => "%{[app][business][event]}"
          "business_value" => "%{[app][business][value]}"
        }
        add_tag => ["business_metric"]
      }
    }
    
    # Set log level
    if [app][level] {
      mutate {
        add_field => { "log_level" => "%{[app][level]}" }
      }
    }
    
    # Add service identification
    mutate {
      add_field => {
        "service_name" => "alchemorsel-api"
        "service_type" => "application"
        "environment" => "production"
      }
      add_tag => ["alchemorsel", "api"]
    }
  }
  
  # Process Database logs (PostgreSQL)
  if [container][name] =~ /postgres/ or [service] == "postgresql" {
    # Parse PostgreSQL log format
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:postgres_timestamp} \[%{NUMBER:pid}\] %{WORD:log_level}:  %{GREEDYDATA:postgres_message}"
      }
    }
    
    # Extract slow queries
    if [postgres_message] =~ /duration: (\d+\.\d+) ms/ {
      grok {
        match => { "postgres_message" => "duration: %{NUMBER:query_duration_ms:float} ms  statement: %{GREEDYDATA:sql_query}" }
      }
      
      # Tag slow queries (> 1000ms)
      if [query_duration_ms] and [query_duration_ms] > 1000 {
        mutate {
          add_tag => ["slow_query"]
        }
      }
    }
    
    # Extract connection information
    if [postgres_message] =~ /connection/ {
      mutate {
        add_tag => ["connection"]
      }
    }
    
    # Extract error information
    if [log_level] in ["ERROR", "FATAL", "PANIC"] {
      mutate {
        add_tag => ["database_error"]
      }
    }
    
    mutate {
      add_field => {
        "service_name" => "postgresql"
        "service_type" => "database"
      }
      add_tag => ["database", "postgresql"]
    }
  }
  
  # Process Redis logs
  if [container][name] =~ /redis/ or [service] == "redis" {
    # Parse Redis log format
    grok {
      match => { 
        "message" => "%{NUMBER:redis_pid}:%{WORD:redis_role} %{TIMESTAMP_ISO8601:redis_timestamp} %{WORD:log_level} %{GREEDYDATA:redis_message}"
      }
    }
    
    # Extract command information
    if [redis_message] =~ /\"[A-Z]+\"/ {
      grok {
        match => { "redis_message" => "\"(?<redis_command>[A-Z]+)\"" }
      }
      mutate {
        add_tag => ["redis_command"]
      }
    }
    
    mutate {
      add_field => {
        "service_name" => "redis"
        "service_type" => "cache"
      }
      add_tag => ["cache", "redis"]
    }
  }
  
  # Process AI Service logs (Ollama)
  if [container][name] =~ /ollama/ or [service] == "ollama" {
    # Parse AI request logs
    json {
      source => "message"
      target => "ai"
    }
    
    if [ai][model] {
      mutate {
        add_field => {
          "ai_model" => "%{[ai][model]}"
          "ai_provider" => "%{[ai][provider]}"
          "ai_request_id" => "%{[ai][request_id]}"
        }
      }
      
      if [ai][duration_ms] {
        mutate {
          add_field => { "ai_duration_ms" => "%{[ai][duration_ms]}" }
          convert => { "ai_duration_ms" => "float" }
        }
      }
      
      if [ai][cost] {
        mutate {
          add_field => { "ai_cost_usd" => "%{[ai][cost]}" }
          convert => { "ai_cost_usd" => "float" }
        }
        mutate {
          add_tag => ["ai_cost"]
        }
      }
    }
    
    mutate {
      add_field => {
        "service_name" => "ollama"
        "service_type" => "ai"
      }
      add_tag => ["ai", "ollama"]
    }
  }
  
  # Process Nginx/Load Balancer logs
  if [container][name] =~ /nginx/ or [service] == "nginx" {
    # Parse Nginx access log format
    grok {
      match => { 
        "message" => "%{NGINXACCESS}"
      }
    }
    
    # GeoIP lookup for client IPs
    if [clientip] {
      geoip {
        source => "clientip"
        target => "geoip"
      }
    }
    
    mutate {
      add_field => {
        "service_name" => "nginx"
        "service_type" => "proxy"
      }
      add_tag => ["proxy", "nginx"]
    }
  }
  
  # Security event processing
  if "security" in [tags] or [log_level] == "SECURITY" {
    mutate {
      add_tag => ["security_event"]
    }
    
    # Extract security event details
    if [app][security] {
      mutate {
        add_field => {
          "security_event_type" => "%{[app][security][event_type]}"
          "security_severity" => "%{[app][security][severity]}"
          "security_source_ip" => "%{[app][security][source_ip]}"
          "security_user_agent" => "%{[app][security][user_agent]}"
          "security_description" => "%{[app][security][description]}"
        }
      }
    }
  }
  
  # Performance event processing
  if "performance" in [tags] or [response_time_ms] {
    if [response_time_ms] and [response_time_ms] > 1000 {
      mutate {
        add_tag => ["slow_request"]
      }
    }
    
    if [response_time_ms] and [response_time_ms] > 5000 {
      mutate {
        add_tag => ["very_slow_request"]
      }
    }
  }
  
  # Error categorization
  if [http_status] {
    if [http_status] >= 400 and [http_status] < 500 {
      mutate {
        add_tag => ["client_error"]
        add_field => { "error_category" => "client_error" }
      }
    } else if [http_status] >= 500 {
      mutate {
        add_tag => ["server_error"]
        add_field => { "error_category" => "server_error" }
      }
    }
  }
  
  # Clean up fields
  mutate {
    remove_field => ["agent", "docker", "app", "ai", "host"]
  }
  
  # Add correlation ID for easier log tracking
  if ![correlation_id] {
    if [trace_id] {
      mutate {
        add_field => { "correlation_id" => "%{trace_id}" }
      }
    } else {
      ruby {
        code => "event.set('correlation_id', SecureRandom.uuid)"
      }
    }
  }
  
  # Add processing timestamp
  mutate {
    add_field => { "processed_at" => "%{@timestamp}" }
  }
}

output {
  # Send to Elasticsearch with index patterns
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    
    # Use different indices based on log type
    if "error" in [tags] {
      index => "alchemorsel-errors-%{+YYYY.MM.dd}"
    } else if "security_event" in [tags] {
      index => "alchemorsel-security-%{+YYYY.MM.dd}"
    } else if "business_metric" in [tags] {
      index => "alchemorsel-business-%{+YYYY.MM.dd}"
    } else if "slow_query" in [tags] {
      index => "alchemorsel-performance-%{+YYYY.MM.dd}"
    } else {
      index => "alchemorsel-logs-%{+YYYY.MM.dd}"
    }
    
    # Template for index settings
    template_name => "alchemorsel"
    template => "/usr/share/logstash/templates/alchemorsel-template.json"
    template_overwrite => true
    
    # Document type
    document_type => "_doc"
  }
  
  # Debug output to stdout in development
  if [environment] == "development" {
    stdout {
      codec => rubydebug
    }
  }
  
  # Send critical errors to dead letter queue
  if "error" in [tags] and [log_level] in ["ERROR", "FATAL"] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "alchemorsel-dlq-%{+YYYY.MM.dd}"
    }
  }
}